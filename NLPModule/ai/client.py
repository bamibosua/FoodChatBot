# NLPModule/ai/client.py (ÄÃƒ CHUYá»‚N HOÃ€N TOÃ€N SANG GROQ)
from groq import Groq
import google.generativeai as genai
from Utils.key_manager import get_gemini_keys, get_groq_keys, get_groq_smart_model, get_groq_fast_model, get_gemini_model

def generate_response(prompt: str, system_instruction: str = "You are a helpful assistant.", model_type="smart"):
    """
    HÃ m gá»i AI sá»­ dá»¥ng Groq vá»›i cÆ¡ cháº¿ xoay vÃ²ng Key vÃ  chá»n model linh hoáº¡t.
    model_type: "smart" (70b - máº·c Ä‘á»‹nh) hoáº·c "fast" (8b)
    """
    keys = get_groq_keys()
    
    # Chá»n model dá»±a trÃªn tham sá»‘ truyá»n vÃ o
    if model_type == "fast":
        model_name = get_groq_fast_model()
    else:
        model_name = get_groq_smart_model()
    
    for i, key in enumerate(keys):
        try:
            client = Groq(api_key=key)
            
            chat_completion = client.chat.completions.create(
                messages=[
                    {"role": "system", "content": system_instruction},
                    {"role": "user", "content": prompt}
                ],
                model=model_name,
                temperature=0.6,
                max_tokens=4096,
                top_p=1,
                stream=False,
            )
            return chat_completion.choices[0].message.content

        except Exception as e:
            # print(f"âš ï¸ [Groq-{model_name}] Key #{i+1} lá»—i: {e}") 
            continue 

    return "Há»‡ thá»‘ng Ä‘ang báº­n, vui lÃ²ng thá»­ láº¡i sau giÃ¢y lÃ¡t."

def gemini_generate_response(prompt: str):
    """
    HÃ m gá»i AI 'Báº¥t tá»­': Tá»± Ä‘á»™ng Ä‘á»•i key náº¿u key hiá»‡n táº¡i háº¿t quota.
    """
    keys = get_gemini_keys() # Láº¥y danh sÃ¡ch key
    
    for i, key in enumerate(keys):
        try:
            # 1. Cáº¥u hÃ¬nh láº¡i vá»›i key má»›i trong vÃ²ng láº·p
            genai.configure(api_key=key)
            
            # 2. Khá»Ÿi táº¡o model
            model_instance = genai.GenerativeModel(get_gemini_model())
            
            # 3. Gá»i API
            # ThÃªm generation_config Ä‘á»ƒ Ä‘áº£m báº£o tráº£ vá» text á»•n Ä‘á»‹nh, khÃ´ng bá»‹ block
            response = model_instance.generate_content(
                prompt,
                generation_config={"temperature": 0.7} 
            )
            
            # 4. ThÃ nh cÃ´ng -> Tráº£ vá» text ngay
            return response.text

        except Exception as e:
            # 5. Náº¿u lá»—i -> In ra console Ä‘á»ƒ biáº¿t vÃ  thá»­ key tiáº¿p theo
            print(f"âš ï¸ [Client AI] Key #{i+1} lá»—i: {e}")
            print(f"ğŸ”„ Äang chuyá»ƒn sang Key dá»± phÃ²ng...")
            continue 

    # Náº¿u thá»­ háº¿t key mÃ  váº«n lá»—i
    return "Xin lá»—i, há»‡ thá»‘ng AI Ä‘ang quÃ¡ táº£i. Vui lÃ²ng thá»­ láº¡i sau giÃ¢y lÃ¡t."